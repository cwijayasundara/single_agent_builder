# Evaluation-Enabled Agent Configuration
agent:
  name: "Evaluation Demo Agent"
  description: "AI agent with comprehensive evaluation capabilities for testing and optimization"
  version: "1.0.0"

# LLM Configuration
llm:
  provider: "openai"
  model: "gpt-4.1-mini"
  temperature: 0.3
  max_tokens: 2000
  api_key_env: "OPENAI_API_KEY"

# Prompts Configuration
prompts:
  system_prompt:
    template: |
      You are an AI assistant designed to be evaluated on multiple dimensions.
      Your responses should be:
      - Accurate and factual
      - Helpful and relevant to the user's query
      - Clear and well-structured
      - Appropriate in tone and style
      
      Current query: {query}
      Memory context: {memory_context}
    variables: ["query", "memory_context"]
    
  user_prompt:
    template: |
      User Request: {user_input}
      
      Please provide a comprehensive and helpful response.
    variables: ["user_input"]

# Tools Configuration
tools:
  built_in:
    - "web_search"
    - "calculator"
    - "file_reader"
  custom: []

# Memory Configuration
memory:
  enabled: true
  provider: "langmem"
  types:
    semantic: true
    episodic: true
    procedural: false
  storage:
    backend: "memory"
  settings:
    max_memory_size: 1000
    retention_days: 7
    background_processing: true

# ReAct Configuration
react:
  max_iterations: 8
  recursion_limit: 40

# Optimization Configuration
optimization:
  enabled: true
  prompt_optimization:
    enabled: true
    feedback_collection: true
    ab_testing: true
    optimization_frequency: "weekly"
  performance_tracking:
    enabled: true
    metrics:
      - "response_time"
      - "accuracy"
      - "user_satisfaction"

# Runtime Configuration
runtime:
  max_iterations: 8
  timeout_seconds: 60
  retry_attempts: 3
  debug_mode: false

# Evaluation Configuration
evaluation:
  enabled: true
  langsmith:
    enabled: true
    api_key_env: "LANGSMITH_API_KEY"
    project_name: "evaluation_demo_agent"
    endpoint: "https://api.smith.langchain.com"
    tracing: true
  evaluators:
    - name: "correctness"
      type: "llm_as_judge"
      prompt: |
        Evaluate the factual correctness and accuracy of the AI response.
        
        Criteria:
        1. Factual accuracy - Are the facts stated correct?
        2. Logical consistency - Is the reasoning sound?
        3. Completeness - Does it adequately address the question?
        4. Up-to-date information - Is the information current?
        
        Score from 0.0 (completely incorrect) to 1.0 (perfectly correct).
        Provide detailed reasoning for your score.
      model: "openai:gpt-4.1-mini"
      parameters:
        temperature: 0.0
      enabled: true
      
    - name: "helpfulness"
      type: "llm_as_judge"
      prompt: |
        Evaluate how helpful this response is to the user.
        
        Criteria:
        1. Relevance - Does it directly address the user's needs?
        2. Actionability - Can the user act on this information?
        3. Clarity - Is it easy to understand?
        4. Thoroughness - Is sufficient detail provided?
        5. User experience - Is it engaging and well-formatted?
        
        Score from 0.0 (not helpful) to 1.0 (extremely helpful).
        Explain your reasoning in detail.
      model: "openai:gpt-4.1-mini"
      parameters:
        temperature: 0.0
      enabled: true
      
    - name: "response_time"
      type: "heuristic"
      parameters:
        target_time: 5.0  # Ideal response time in seconds
        max_time: 20.0    # Maximum acceptable time
      enabled: true
      
    - name: "tool_usage"
      type: "heuristic"
      parameters:
        efficiency_weight: 0.7
        accuracy_weight: 0.3
      enabled: true
      
    - name: "coherence"
      type: "llm_as_judge"
      prompt: |
        Evaluate the coherence and structure of the AI response.
        
        Criteria:
        1. Logical flow - Do ideas connect logically?
        2. Organization - Is information well-organized?
        3. Consistency - Is the tone and style consistent?
        4. Readability - Is it easy to follow?
        
        Score from 0.0 (incoherent) to 1.0 (perfectly structured).
      model: "openai:gpt-4.1-mini"
      enabled: true
      
  datasets:
    - name: "general_qa_test"
      description: "General Q&A test cases for comprehensive evaluation"
      examples:
        - inputs:
            query: "What is the capital of France?"
          outputs:
            answer: "The capital of France is Paris."
          metadata:
            category: "factual"
            difficulty: "easy"
            
        - inputs:
            query: "Explain how photosynthesis works"
          outputs:
            answer: "Photosynthesis is the process by which plants convert sunlight, carbon dioxide, and water into glucose and oxygen using chlorophyll."
          metadata:
            category: "scientific"
            difficulty: "medium"
            
        - inputs:
            query: "What are the pros and cons of remote work?"
          outputs:
            answer: "Remote work offers flexibility and reduced commuting but can lead to isolation and communication challenges."
          metadata:
            category: "analytical"
            difficulty: "medium"
            
        - inputs:
            query: "Calculate the area of a circle with radius 5"
          outputs:
            answer: "The area of a circle with radius 5 is approximately 78.54 square units (π × 5² = 25π ≈ 78.54)."
          metadata:
            category: "mathematical"
            difficulty: "easy"
            
    - name: "complex_reasoning_test"
      description: "Complex reasoning and multi-step problem solving"
      examples:
        - inputs:
            query: "If I invest $1000 at 5% annual interest compounded monthly, how much will I have after 2 years?"
          outputs:
            answer: "After 2 years, you would have approximately $1104.94."
          metadata:
            category: "financial_calculation"
            difficulty: "hard"
            
        - inputs:
            query: "Compare the environmental impact of electric cars vs gasoline cars"
          outputs:
            answer: "Electric cars generally have lower lifetime emissions despite higher manufacturing emissions, especially when powered by renewable energy."
          metadata:
            category: "comparative_analysis"
            difficulty: "hard"
            
  metrics: ["correctness", "helpfulness", "response_time", "tool_usage", "coherence", "user_satisfaction"]
  auto_evaluate: true
  evaluation_frequency: "per_run"  # Evaluate every run for demo purposes
  batch_size: 10
  max_concurrency: 3

# Logging Configuration
logging:
  enabled: true
  level: "INFO"
  format: "structured"
  console:
    enabled: true
    level: "INFO"
    format: "readable"
  file:
    enabled: true
    path: "logs/evaluation_demo_agent.log"
    level: "DEBUG"
    max_size_mb: 150
    backup_count: 7
  components:
    agent: "INFO"
    evaluation: "DEBUG"  # Detailed evaluation logging
    tools: "INFO"
    memory: "INFO"
    optimization: "INFO"
  correlation:
    enabled: true
    include_in_response: true  # Helpful for evaluation tracking
  performance:
    log_execution_time: true
    log_token_usage: true
    log_memory_operations: true
  privacy:
    mask_api_keys: true
    mask_user_input: false
    excluded_fields: ["api_key", "auth_token"]